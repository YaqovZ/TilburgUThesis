{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2.2\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacobzhao/anaconda3/envs/prophet/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Importing plotly failed. Interactive plots will not work.\n",
      "2024-06-02 11:04:00.611241: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import darts\n",
    "from darts import TimeSeries\n",
    "from darts.models import NaiveSeasonal, NaiveMean, NaiveDrift\n",
    "from darts.models import StatsForecastAutoARIMA, StatsForecastAutoETS, StatsForecastAutoCES, RNNModel, ExponentialSmoothing, BlockRNNModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "from darts.models import RNNModel, ExponentialSmoothing, BlockRNNModel\n",
    "from darts.metrics import mape, mase, mse, rmse, ase, ape, r2_score, smape\n",
    "from darts.utils.statistics import check_seasonality, plot_acf\n",
    "from darts.datasets import AirPassengersDataset, SunspotsDataset\n",
    "from darts.utils.timeseries_generation import datetime_attribute_timeseries\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dropout, Dense, Flatten\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import logging\n",
    "logging.disable(logging.CRITICAL)\n",
    "import random\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD, Nadam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error, mean_absolute_error\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "import os\n",
    "\n",
    "from hyperopt import base\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "os.environ['HYPEROPT_FMIN_SEED'] = \"1\"\n",
    "random.seed(88)\n",
    "np.random.seed(88)\n",
    "tf.random.set_seed(88)\n",
    "base.have_been_bugged = False  \n",
    "rstate = np.random.default_rng(88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total = pd.read_csv('time_series_thesis_question_1.csv', index_col=0)\n",
    "df_total.index = pd.to_datetime(df_total.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = TimeSeries.from_dataframe(df_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_1, val_1 = df.split_before(pd.Timestamp('20230101'))\n",
    "train_2, val_2 = df.split_before(pd.Timestamp('20230401'))\n",
    "train_3, val_3 = df.split_before(pd.Timestamp('20230701'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Xy(sliding_windows):\n",
    "    X = [[list(window[:-1]) for window in windows] for windows in sliding_windows]\n",
    "    y = [[window[-1] for window in windows] for windows in sliding_windows]\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_percentage_error, mean_absolute_error\n",
    "\n",
    "def spliter(df_total,\n",
    "            k = 4,\n",
    "            test_size = 3,\n",
    "            val_size = 3):\n",
    "    test = []\n",
    "    val = []\n",
    "    train = []\n",
    "    NN_sets = {}\n",
    "    window_size = k+1\n",
    "\n",
    "\n",
    "    for col in df_total.columns:\n",
    "        windows = [np.array(window) for window in df_total[col].rolling(window_size) if len(window) == window_size]\n",
    "        test.append(windows[-(test_size):])\n",
    "        val.append(windows[-(test_size+val_size):-(test_size)])\n",
    "        train.append(windows[:-(test_size+val_size)])\n",
    "    NN_sets['X_train'], NN_sets['y_train'] = Xy(train)\n",
    "    NN_sets['X_val'], NN_sets['y_val'] = Xy(val)\n",
    "    NN_sets['X_test'], NN_sets['y_test'] = Xy(test)\n",
    "    return NN_sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import darts\n",
    "def NN_metricker(y_pred):\n",
    "  y_pred_df = pd.DataFrame(y_pred.reshape((-1, 3)).transpose())\n",
    "  y_pred_df.columns = df_total.columns\n",
    "  y_pred_df.index = df_total.index[-3:]\n",
    "  y_pred_df.index = pd.to_datetime(y_pred_df.index)\n",
    "  y_pred_tf = TimeSeries.from_dataframe(y_pred_df)\n",
    "  SMAPE = darts.metrics.smape(val_1, y_pred_tf)\n",
    "  MASE = darts.metrics.mase(val_1, y_pred_tf, train_1)\n",
    "  MAE = darts.metrics.mae(val_1, y_pred_tf)\n",
    "  print(\n",
    "      \"Symmetric Mean absolute percentage error: {:.2f}%.\".format(\n",
    "          SMAPE),\n",
    "          \"MASE: {:.2f}\".format(MASE),\n",
    "          \"MAE: {:.2f}\".format(MAE)\n",
    "      )\n",
    "  return y_pred_df, SMAPE, MASE, MAE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total = pd.read_csv('time_series_thesis_question_1.csv', index_col=0)\n",
    "K.clear_session()\n",
    "tf.keras.backend.clear_session()\n",
    "from tensorflow.keras.losses import Huber\n",
    "def create_lstm_model(window, lstm_units, n_lstm_layers, optimizer_name, lr, dp, df=df_total):\n",
    "    NN_sets = spliter(df, k=window)  # Assuming 'spliter' and 'df_total' are defined\n",
    "\n",
    "    lstm_model = Sequential()\n",
    "    if n_lstm_layers == 1:\n",
    "        lstm_model.add(LSTM(lstm_units, return_sequences=False, input_shape=(window,1)))\n",
    "    elif n_lstm_layers == 2:\n",
    "        lstm_model.add(LSTM(lstm_units, return_sequences=True, input_shape=(window,1)))\n",
    "        lstm_model.add(LSTM(lstm_units // 2, return_sequences=False))\n",
    "    elif n_lstm_layers == 3:\n",
    "        lstm_model.add(LSTM(lstm_units, return_sequences=True, input_shape=(window,1)))\n",
    "        lstm_model.add(LSTM(lstm_units // 2, return_sequences=True))\n",
    "        lstm_model.add(LSTM(lstm_units // 4, return_sequences=False))\n",
    "    \n",
    "    \n",
    "    lstm_model.add(Dropout(dp))\n",
    "    lstm_model.add(Dense(units=1))\n",
    "\n",
    "    optimizer_class = {'adam': Adam, 'rmsprop': RMSprop, 'sgd': SGD, 'nadam': Nadam}[optimizer_name]\n",
    "    optimizer = optimizer_class(lr)\n",
    "\n",
    "    lstm_model.compile(optimizer=optimizer, loss=Huber())\n",
    "\n",
    "\n",
    "    return lstm_model, NN_sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters to tune\n",
    "window_values = [16, 20, 24, 28]\n",
    "lstm_units_values = [16, 32, 64, 128]\n",
    "n_lstm_layers_values = [1, 2, 3]\n",
    "optimizer_values = ['adam', 'rmsprop', 'nadam']\n",
    "lr_values = list(np.arange(1e-4, 11e-4, 1e-4))\n",
    "dropout_values = list(np.arange(0, 0.5, 0.1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replicate Results from LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [4:21:29<00:00, 156.89s/trial, best loss: 0.8967740535736084] \n",
      "Best hyperparameters:\n",
      "{'dropout_value': 0, 'lr': 7, 'lstm_units': 3, 'n_lstm_layers': 0, 'optimizer': 2, 'window': 1}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD, Nadam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Set random seeds\n",
    "seed = 88\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# Hyperparameters to tune\n",
    "window_values = [16, 20, 24, 28]\n",
    "lstm_units_values = [16, 32, 64, 128]\n",
    "n_lstm_layers_values = [1, 2, 3]\n",
    "optimizer_values = ['adam', 'rmsprop', 'nadam']\n",
    "lr_values = list(np.arange(1e-4, 11e-4, 1e-4))\n",
    "dropout_values = list(np.arange(0, 0.5, 0.1))\n",
    "# Define the model creation function\n",
    "\n",
    "trials_results = []\n",
    "\n",
    "# Define the objective function for Hyperopt\n",
    "def objective(params):\n",
    "    window = int(params['window'])\n",
    "    lstm_units = int(params['lstm_units'])\n",
    "    n_lstm_layers = int(params['n_lstm_layers'])\n",
    "    optimizer_name = params['optimizer']\n",
    "    lr = params['lr']\n",
    "    dropout_value = params['dropout_value']\n",
    "    model, NN_sets = create_lstm_model(window, lstm_units, n_lstm_layers, optimizer_name, lr, dropout_value, df=df_total)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "\n",
    "    # Reshape the input data\n",
    "    X_train = NN_sets['X_train'].reshape((-1, window, 1))\n",
    "    y_train = NN_sets['y_train'].reshape((-1, 1))\n",
    "    X_val = NN_sets['X_val'].reshape((-1, window, 1))\n",
    "    y_val = NN_sets['y_val'].reshape((-1, 1))\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        epochs=30,\n",
    "        batch_size=16,\n",
    "        validation_data=(X_val, y_val),\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    val_loss = history.history['val_loss'][-1]\n",
    "    trials_results.append({'params': params, 'val_loss': val_loss})\n",
    "\n",
    "    return {'loss': val_loss, 'status': STATUS_OK}\n",
    "\n",
    "# Define the hyperparameter search space\n",
    "search_space = {\n",
    "    'window': hp.choice('window', window_values),\n",
    "    'n_lstm_layers': hp.choice('n_lstm_layers', n_lstm_layers_values),\n",
    "    'lstm_units': hp.choice('lstm_units', lstm_units_values),\n",
    "    'optimizer': hp.choice('optimizer', optimizer_values),\n",
    "    'lr': hp.choice('lr', lr_values),\n",
    "    'dropout_value': hp.choice('dropout_value', dropout_values)\n",
    "}\n",
    "\n",
    "# Conduct the Bayesian optimization\n",
    "trials = Trials()\n",
    "best = fmin(objective, search_space, algo=tpe.suggest, max_evals=100, trials=trials, rstate=rstate)\n",
    "\n",
    "# Print the best hyperparameters and the corresponding validation loss\n",
    "print('Best hyperparameters:')\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming trials_results is your list of dictionaries\n",
    "df = pd.DataFrame(trials_results)\n",
    "\n",
    "# If you want to flatten the 'params' column into separate columns\n",
    "df = pd.concat([df.drop(['params'], axis=1), df['params'].apply(pd.Series)], axis=1)\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "if not os.path.exists('LSTM_results'):\n",
    "    os.makedirs('LSTM_results')\n",
    "\n",
    "# Write df to a csv file in the specified directory\n",
    "df.to_csv('LSTM_results/LSTM_tuning_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best window size: 20\n",
      "Best number of LSTM layers: 1\n",
      "Best number of LSTM units: 128\n",
      "Best optimizer: nadam\n",
      "Best learning rate: 0.0008\n",
      "Best dropout rate: 0.0\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "tf.keras.backend.clear_session()\n",
    "# Map the indices back to the actual values\n",
    "best_window = window_values[best['window']]\n",
    "best_n_lstm_layers = n_lstm_layers_values[best['n_lstm_layers']]\n",
    "best_lstm_units = lstm_units_values[best['lstm_units']]\n",
    "best_optimizer_name =  optimizer_values[best['optimizer']]\n",
    "best_lr = lr_values[best['lr']]\n",
    "best_dropout_value = dropout_values[best['dropout_value']]\n",
    "print(f\"Best window size: {best_window}\")\n",
    "print(f\"Best number of LSTM layers: {best_n_lstm_layers}\")\n",
    "print(f\"Best number of LSTM units: {best_lstm_units}\")\n",
    "print(f\"Best optimizer: {best_optimizer_name}\")\n",
    "print(f\"Best learning rate: {best_lr}\")\n",
    "print(f\"Best dropout rate: {best_dropout_value}\")\n",
    "best_model, NN_sets = create_lstm_model(best_window, best_lstm_units, best_n_lstm_layers, best_optimizer_name, best_lr, best_dropout_value, df=df_total)\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "history = best_model.fit(\n",
    "    NN_sets['X_train'].reshape((-1, best_window, 1)),\n",
    "    NN_sets['y_train'].reshape((-1, 1)),\n",
    "    epochs=30,\n",
    "    batch_size=16,\n",
    "    validation_data=(NN_sets['X_val'].reshape((-1, best_window, 1)), NN_sets['y_val'].reshape((-1, 1))),\n",
    "    callbacks=[early_stop], verbose=0\n",
    ")\n",
    "y_pred = best_model.predict(NN_sets['X_test'].reshape((-1, best_window, 1)))\n",
    "y_pred_df, SMAPE, MASE, MAE = NN_metricker(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73/73 [==============================] - 0s 4ms/step\n",
      "Symmetric Mean absolute percentage error: 14.53%. MASE: 1.14 MAE: 1.14\n"
     ]
    }
   ],
   "source": [
    "y_pred = best_model.predict(NN_sets['X_test'].reshape((-1, best_window, 1)))\n",
    "y_pred_df, SMAPE, MASE, MAE = NN_metricker(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18446.900973256765\n"
     ]
    }
   ],
   "source": [
    " def mimicry_generator(y_pred_df):\n",
    "    z_preceding = df_total[-4:-1].reset_index()\n",
    "    z_preceding.drop(z_preceding.columns[0], axis=1, inplace=True)\n",
    "    z_present = df_total[-3:].reset_index()\n",
    "    z_present.drop(z_present.columns[0], axis=1, inplace=True)\n",
    "    z_pred = y_pred_df.reset_index()\n",
    "    z_pred.drop(z_pred.columns[0], axis=1, inplace=True)\n",
    "    mimic = np.square(z_present - z_pred) - np.square(z_preceding - z_pred) # large means mimickry\n",
    "    return mimic, mimic.sum().sum()\n",
    "mimic, mimic_index = mimicry_generator(df_total[-4:-1])\n",
    "print(mimic_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Normalized Mimicry Sum:\n",
      "0.8954372542032915\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def mimicry_generator(y_pred_df):\n",
    "    z_preceding = df_total[-4:-1].reset_index()\n",
    "    z_preceding.drop(z_preceding.columns[0], axis=1, inplace=True)\n",
    "    z_present = df_total[-3:].reset_index()\n",
    "    z_present.drop(z_present.columns[0], axis=1, inplace=True)\n",
    "    z_pred = y_pred_df.reset_index()\n",
    "    z_pred.drop(z_pred.columns[0], axis=1, inplace=True)\n",
    "    mimic = np.square(z_present - z_pred) - np.square(z_preceding - z_pred) # large means mimickry\n",
    "    return mimic, mimic.sum().sum()\n",
    "\n",
    "def normalize_mimicry(mimic_sum, mimic_sum_correct, mimic_sum_mimicry):\n",
    "    # 非线性归一化，考虑平方关系\n",
    "    normalized_mimic_sum = np.sqrt((mimic_sum - mimic_sum_correct) / (mimic_sum_mimicry - mimic_sum_correct))\n",
    "    \n",
    "    return normalized_mimic_sum\n",
    "\n",
    "\n",
    "# 完全模仿最后一个输入值\n",
    "y_pred_df_mimicry = df_total[-4:-1]\n",
    "_, mimic_sum_mimicry = mimicry_generator(y_pred_df_mimicry)\n",
    "\n",
    "# 完全正确预测\n",
    "y_pred_df_correct = df_total[-3:]\n",
    "_, mimic_sum_correct = mimicry_generator(y_pred_df_correct)\n",
    "\n",
    "# 归一化函数\n",
    "def mimicry_generator_normalized(y_pred_df, mimic_sum_correct, mimic_sum_mimicry):\n",
    "    mimic, mimic_sum = mimicry_generator(y_pred_df)\n",
    "    normalized_mimic_sum = normalize_mimicry(mimic_sum, mimic_sum_correct, mimic_sum_mimicry)\n",
    "    \n",
    "    return mimic, normalized_mimic_sum\n",
    "\n",
    "\n",
    "\n",
    "mimic, normalized_mimic_sum = mimicry_generator_normalized(y_pred_df[0], mimic_sum_correct, mimic_sum_mimicry)\n",
    "\n",
    "\n",
    "print(\"\\nNormalized Mimicry Sum:\")\n",
    "print(normalized_mimic_sum)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = {'dropout_value': 0, 'lr': 7, 'lstm_units': 3, 'n_lstm_layers': 0, 'optimizer': 2, 'window': 1}\n",
    "\n",
    "best_window = window_values[best['window']]\n",
    "best_n_lstm_layers = n_lstm_layers_values[best['n_lstm_layers']]\n",
    "best_lstm_units = lstm_units_values[best['lstm_units']]\n",
    "best_optimizer_name =  optimizer_values[best['optimizer']]\n",
    "best_lr = lr_values[best['lr']]\n",
    "best_dropout_value = dropout_values[best['dropout_value']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "628/628 [==============================] - 7s 11ms/step - loss: 0.6963 - val_loss: 0.8963\n",
      "73/73 [==============================] - 1s 6ms/step\n",
      "Symmetric Mean absolute percentage error: 14.51%. MASE: 1.13 MAE: 1.14\n",
      "Epoch 1/30\n",
      "628/628 [==============================] - 10s 11ms/step - loss: 1.1246 - val_loss: 1.0595\n",
      "Epoch 2/30\n",
      "628/628 [==============================] - 7s 10ms/step - loss: 0.7488 - val_loss: 0.9635\n",
      "Epoch 3/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7231 - val_loss: 0.9836\n",
      "Epoch 4/30\n",
      "628/628 [==============================] - 7s 10ms/step - loss: 0.7159 - val_loss: 0.9078\n",
      "Epoch 5/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7107 - val_loss: 0.9209\n",
      "Epoch 6/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7118 - val_loss: 0.9442\n",
      "Epoch 7/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7085 - val_loss: 0.9107\n",
      "Epoch 8/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7042 - val_loss: 0.9121\n",
      "Epoch 9/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7053 - val_loss: 0.9401\n",
      "73/73 [==============================] - 1s 6ms/step\n",
      "Symmetric Mean absolute percentage error: 15.15%. MASE: 1.20 MAE: 1.17\n",
      "Epoch 1/30\n",
      "628/628 [==============================] - 10s 11ms/step - loss: 1.1845 - val_loss: 1.0660\n",
      "Epoch 2/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7490 - val_loss: 0.9672\n",
      "Epoch 3/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7242 - val_loss: 0.9772\n",
      "Epoch 4/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7171 - val_loss: 0.9116\n",
      "Epoch 5/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7112 - val_loss: 0.9237\n",
      "Epoch 6/30\n",
      "628/628 [==============================] - 6s 9ms/step - loss: 0.7127 - val_loss: 0.9339\n",
      "Epoch 7/30\n",
      "628/628 [==============================] - 6s 9ms/step - loss: 0.7098 - val_loss: 0.9054\n",
      "Epoch 8/30\n",
      "628/628 [==============================] - 6s 10ms/step - loss: 0.7056 - val_loss: 0.9190\n",
      "Epoch 9/30\n",
      "628/628 [==============================] - 6s 10ms/step - loss: 0.7062 - val_loss: 0.9421\n",
      "Epoch 10/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7061 - val_loss: 0.9164\n",
      "Epoch 11/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7065 - val_loss: 0.8957\n",
      "Epoch 12/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7009 - val_loss: 0.9107\n",
      "Epoch 13/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.6986 - val_loss: 0.9087\n",
      "Epoch 14/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7021 - val_loss: 0.9110\n",
      "Epoch 15/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.6985 - val_loss: 0.9059\n",
      "Epoch 16/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.6960 - val_loss: 0.8965\n",
      "73/73 [==============================] - 1s 5ms/step\n",
      "Symmetric Mean absolute percentage error: 14.48%. MASE: 1.13 MAE: 1.13\n",
      "Epoch 1/30\n",
      "628/628 [==============================] - 10s 11ms/step - loss: 1.1206 - val_loss: 1.0499\n",
      "Epoch 2/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7462 - val_loss: 0.9665\n",
      "Epoch 3/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7240 - val_loss: 0.9814\n",
      "Epoch 4/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7174 - val_loss: 0.9105\n",
      "Epoch 5/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7123 - val_loss: 0.9215\n",
      "Epoch 6/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7128 - val_loss: 0.9414\n",
      "Epoch 7/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7097 - val_loss: 0.9049\n",
      "Epoch 8/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7052 - val_loss: 0.9121\n",
      "Epoch 9/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7055 - val_loss: 0.9315\n",
      "Epoch 10/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7061 - val_loss: 0.9066\n",
      "Epoch 11/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7053 - val_loss: 0.8930\n",
      "Epoch 12/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7005 - val_loss: 0.9059\n",
      "Epoch 13/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.6978 - val_loss: 0.9069\n",
      "Epoch 14/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7008 - val_loss: 0.9075\n",
      "Epoch 15/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.6976 - val_loss: 0.9006\n",
      "Epoch 16/30\n",
      "628/628 [==============================] - 6s 9ms/step - loss: 0.6950 - val_loss: 0.8891\n",
      "Epoch 17/30\n",
      "628/628 [==============================] - 6s 10ms/step - loss: 0.6971 - val_loss: 0.8915\n",
      "Epoch 18/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.6953 - val_loss: 0.9017\n",
      "Epoch 19/30\n",
      "628/628 [==============================] - 8s 13ms/step - loss: 0.6937 - val_loss: 0.9090\n",
      "Epoch 20/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.6917 - val_loss: 0.8934\n",
      "Epoch 21/30\n",
      "628/628 [==============================] - 7s 12ms/step - loss: 0.6926 - val_loss: 0.9124\n",
      "73/73 [==============================] - 1s 6ms/step\n",
      "Symmetric Mean absolute percentage error: 14.46%. MASE: 1.13 MAE: 1.13\n",
      "Epoch 1/30\n",
      "628/628 [==============================] - 11s 12ms/step - loss: 1.1072 - val_loss: 1.0644\n",
      "Epoch 2/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7477 - val_loss: 0.9594\n",
      "Epoch 3/30\n",
      "628/628 [==============================] - 7s 10ms/step - loss: 0.7234 - val_loss: 0.9819\n",
      "Epoch 4/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7161 - val_loss: 0.9106\n",
      "Epoch 5/30\n",
      "628/628 [==============================] - 8s 12ms/step - loss: 0.7116 - val_loss: 0.9219\n",
      "Epoch 6/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7128 - val_loss: 0.9391\n",
      "Epoch 7/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7096 - val_loss: 0.9026\n",
      "Epoch 8/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7046 - val_loss: 0.9191\n",
      "Epoch 9/30\n",
      "628/628 [==============================] - 7s 10ms/step - loss: 0.7063 - val_loss: 0.9396\n",
      "Epoch 10/30\n",
      "628/628 [==============================] - 7s 10ms/step - loss: 0.7062 - val_loss: 0.8992\n",
      "Epoch 11/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7050 - val_loss: 0.8951\n",
      "Epoch 12/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.6995 - val_loss: 0.9166\n",
      "Epoch 13/30\n",
      "628/628 [==============================] - 6s 9ms/step - loss: 0.6990 - val_loss: 0.9089\n",
      "Epoch 14/30\n",
      "628/628 [==============================] - 5s 9ms/step - loss: 0.7004 - val_loss: 0.9157\n",
      "Epoch 15/30\n",
      "628/628 [==============================] - 6s 10ms/step - loss: 0.6989 - val_loss: 0.9048\n",
      "Epoch 16/30\n",
      "628/628 [==============================] - 7s 10ms/step - loss: 0.6953 - val_loss: 0.9022\n",
      "73/73 [==============================] - 1s 10ms/step\n",
      "Symmetric Mean absolute percentage error: 14.46%. MASE: 1.13 MAE: 1.14\n",
      "Epoch 1/30\n",
      "628/628 [==============================] - 9s 11ms/step - loss: 1.1500 - val_loss: 1.0778\n",
      "Epoch 2/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7516 - val_loss: 0.9716\n",
      "Epoch 3/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7241 - val_loss: 0.9843\n",
      "Epoch 4/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7170 - val_loss: 0.9153\n",
      "Epoch 5/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7131 - val_loss: 0.9279\n",
      "Epoch 6/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7139 - val_loss: 0.9396\n",
      "Epoch 7/30\n",
      "628/628 [==============================] - 7s 10ms/step - loss: 0.7102 - val_loss: 0.9119\n",
      "Epoch 8/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7059 - val_loss: 0.9162\n",
      "Epoch 9/30\n",
      "628/628 [==============================] - 6s 10ms/step - loss: 0.7069 - val_loss: 0.9489\n",
      "Epoch 10/30\n",
      "628/628 [==============================] - 6s 10ms/step - loss: 0.7072 - val_loss: 0.9117\n",
      "Epoch 11/30\n",
      "628/628 [==============================] - 8s 12ms/step - loss: 0.7061 - val_loss: 0.8932\n",
      "Epoch 12/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7006 - val_loss: 0.9027\n",
      "Epoch 13/30\n",
      "628/628 [==============================] - 7s 10ms/step - loss: 0.6978 - val_loss: 0.9053\n",
      "Epoch 14/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7013 - val_loss: 0.9092\n",
      "Epoch 15/30\n",
      "628/628 [==============================] - 6s 10ms/step - loss: 0.6982 - val_loss: 0.9023\n",
      "Epoch 16/30\n",
      "628/628 [==============================] - 6s 10ms/step - loss: 0.6964 - val_loss: 0.8960\n",
      "73/73 [==============================] - 1s 4ms/step\n",
      "Symmetric Mean absolute percentage error: 14.42%. MASE: 1.13 MAE: 1.13\n",
      "Epoch 1/30\n",
      "628/628 [==============================] - 9s 10ms/step - loss: 1.0987 - val_loss: 1.0530\n",
      "Epoch 2/30\n",
      "628/628 [==============================] - 6s 10ms/step - loss: 0.7461 - val_loss: 0.9649\n",
      "Epoch 3/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7229 - val_loss: 0.9890\n",
      "Epoch 4/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7173 - val_loss: 0.9096\n",
      "Epoch 5/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7125 - val_loss: 0.9261\n",
      "Epoch 6/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7127 - val_loss: 0.9426\n",
      "Epoch 7/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7093 - val_loss: 0.9026\n",
      "Epoch 8/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7051 - val_loss: 0.9160\n",
      "Epoch 9/30\n",
      "628/628 [==============================] - 6s 10ms/step - loss: 0.7062 - val_loss: 0.9390\n",
      "Epoch 10/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7060 - val_loss: 0.9140\n",
      "Epoch 11/30\n",
      "628/628 [==============================] - 6s 10ms/step - loss: 0.7059 - val_loss: 0.8945\n",
      "Epoch 12/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7006 - val_loss: 0.9075\n",
      "Epoch 13/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.6989 - val_loss: 0.9055\n",
      "Epoch 14/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7014 - val_loss: 0.9075\n",
      "Epoch 15/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.6974 - val_loss: 0.9010\n",
      "Epoch 16/30\n",
      "628/628 [==============================] - 6s 10ms/step - loss: 0.6939 - val_loss: 0.9002\n",
      "73/73 [==============================] - 1s 5ms/step\n",
      "Symmetric Mean absolute percentage error: 14.54%. MASE: 1.14 MAE: 1.14\n",
      "Epoch 1/30\n",
      "628/628 [==============================] - 10s 11ms/step - loss: 1.1255 - val_loss: 1.0669\n",
      "Epoch 2/30\n",
      "628/628 [==============================] - 5s 8ms/step - loss: 0.7478 - val_loss: 0.9619\n",
      "Epoch 3/30\n",
      "628/628 [==============================] - 6s 10ms/step - loss: 0.7222 - val_loss: 0.9775\n",
      "Epoch 4/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7155 - val_loss: 0.9103\n",
      "Epoch 5/30\n",
      "628/628 [==============================] - 7s 12ms/step - loss: 0.7120 - val_loss: 0.9248\n",
      "Epoch 6/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7133 - val_loss: 0.9529\n",
      "Epoch 7/30\n",
      "628/628 [==============================] - 7s 10ms/step - loss: 0.7104 - val_loss: 0.9051\n",
      "Epoch 8/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7059 - val_loss: 0.9151\n",
      "Epoch 9/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7069 - val_loss: 0.9346\n",
      "Epoch 10/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7073 - val_loss: 0.9161\n",
      "Epoch 11/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7067 - val_loss: 0.8985\n",
      "Epoch 12/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7018 - val_loss: 0.9065\n",
      "Epoch 13/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.6994 - val_loss: 0.9106\n",
      "Epoch 14/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7024 - val_loss: 0.9083\n",
      "Epoch 15/30\n",
      "628/628 [==============================] - 7s 10ms/step - loss: 0.6993 - val_loss: 0.9016\n",
      "Epoch 16/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.6970 - val_loss: 0.8952\n",
      "Epoch 17/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.6979 - val_loss: 0.8973\n",
      "Epoch 18/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.6982 - val_loss: 0.9037\n",
      "Epoch 19/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.6959 - val_loss: 0.9102\n",
      "Epoch 20/30\n",
      "628/628 [==============================] - 6s 10ms/step - loss: 0.6932 - val_loss: 0.8917\n",
      "Epoch 21/30\n",
      "628/628 [==============================] - 6s 9ms/step - loss: 0.6954 - val_loss: 0.9143\n",
      "Epoch 22/30\n",
      "628/628 [==============================] - 6s 9ms/step - loss: 0.6916 - val_loss: 0.9162\n",
      "Epoch 23/30\n",
      "628/628 [==============================] - 7s 10ms/step - loss: 0.6892 - val_loss: 0.9155\n",
      "Epoch 24/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.6893 - val_loss: 0.8987\n",
      "Epoch 25/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.6888 - val_loss: 0.9119\n",
      "73/73 [==============================] - 1s 5ms/step\n",
      "Symmetric Mean absolute percentage error: 14.31%. MASE: 1.12 MAE: 1.13\n",
      "Epoch 1/30\n",
      "628/628 [==============================] - 9s 11ms/step - loss: 1.1334 - val_loss: 1.0524\n",
      "Epoch 2/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7470 - val_loss: 0.9644\n",
      "Epoch 3/30\n",
      "628/628 [==============================] - 6s 10ms/step - loss: 0.7238 - val_loss: 0.9769\n",
      "Epoch 4/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7173 - val_loss: 0.9109\n",
      "Epoch 5/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7117 - val_loss: 0.9220\n",
      "Epoch 6/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7127 - val_loss: 0.9411\n",
      "Epoch 7/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7094 - val_loss: 0.9069\n",
      "Epoch 8/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7045 - val_loss: 0.9185\n",
      "Epoch 9/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7067 - val_loss: 0.9431\n",
      "Epoch 10/30\n",
      "628/628 [==============================] - 6s 10ms/step - loss: 0.7073 - val_loss: 0.8992\n",
      "Epoch 11/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7055 - val_loss: 0.8970\n",
      "Epoch 12/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7004 - val_loss: 0.9113\n",
      "Epoch 13/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.6991 - val_loss: 0.9001\n",
      "Epoch 14/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7024 - val_loss: 0.9062\n",
      "Epoch 15/30\n",
      "628/628 [==============================] - 6s 10ms/step - loss: 0.6985 - val_loss: 0.9061\n",
      "Epoch 16/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.6953 - val_loss: 0.8945\n",
      "Epoch 17/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.6979 - val_loss: 0.8995\n",
      "Epoch 18/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.6965 - val_loss: 0.8999\n",
      "Epoch 19/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.6953 - val_loss: 0.9115\n",
      "Epoch 20/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.6938 - val_loss: 0.8931\n",
      "Epoch 21/30\n",
      "628/628 [==============================] - 6s 10ms/step - loss: 0.6950 - val_loss: 0.9186\n",
      "Epoch 22/30\n",
      "628/628 [==============================] - 5s 8ms/step - loss: 0.6930 - val_loss: 0.8998\n",
      "Epoch 23/30\n",
      "628/628 [==============================] - 6s 10ms/step - loss: 0.6899 - val_loss: 0.9189\n",
      "Epoch 24/30\n",
      "628/628 [==============================] - 7s 10ms/step - loss: 0.6885 - val_loss: 0.9082\n",
      "Epoch 25/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.6886 - val_loss: 0.9167\n",
      "73/73 [==============================] - 1s 5ms/step\n",
      "Symmetric Mean absolute percentage error: 14.28%. MASE: 1.12 MAE: 1.12\n",
      "Epoch 1/30\n",
      "628/628 [==============================] - 9s 11ms/step - loss: 1.1424 - val_loss: 1.0569\n",
      "Epoch 2/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7483 - val_loss: 0.9654\n",
      "Epoch 3/30\n",
      "628/628 [==============================] - 7s 10ms/step - loss: 0.7228 - val_loss: 0.9764\n",
      "Epoch 4/30\n",
      "628/628 [==============================] - 7s 10ms/step - loss: 0.7162 - val_loss: 0.9125\n",
      "Epoch 5/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7113 - val_loss: 0.9233\n",
      "Epoch 6/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7122 - val_loss: 0.9431\n",
      "Epoch 7/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7107 - val_loss: 0.9050\n",
      "Epoch 8/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7057 - val_loss: 0.9167\n",
      "Epoch 9/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7061 - val_loss: 0.9351\n",
      "Epoch 10/30\n",
      "628/628 [==============================] - 6s 10ms/step - loss: 0.7054 - val_loss: 0.9035\n",
      "Epoch 11/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7056 - val_loss: 0.8960\n",
      "Epoch 12/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7000 - val_loss: 0.9058\n",
      "Epoch 13/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.6978 - val_loss: 0.9076\n",
      "Epoch 14/30\n",
      "628/628 [==============================] - 6s 10ms/step - loss: 0.6994 - val_loss: 0.9103\n",
      "Epoch 15/30\n",
      "628/628 [==============================] - 5s 8ms/step - loss: 0.6962 - val_loss: 0.9014\n",
      "Epoch 16/30\n",
      "628/628 [==============================] - 6s 9ms/step - loss: 0.6932 - val_loss: 0.8902\n",
      "Epoch 17/30\n",
      "628/628 [==============================] - 6s 10ms/step - loss: 0.6954 - val_loss: 0.8949\n",
      "Epoch 18/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.6950 - val_loss: 0.9052\n",
      "Epoch 19/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.6934 - val_loss: 0.9097\n",
      "Epoch 20/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.6911 - val_loss: 0.8956\n",
      "Epoch 21/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.6915 - val_loss: 0.9275\n",
      "73/73 [==============================] - 1s 5ms/step\n",
      "Symmetric Mean absolute percentage error: 14.62%. MASE: 1.14 MAE: 1.15\n",
      "Epoch 1/30\n",
      "628/628 [==============================] - 9s 11ms/step - loss: 1.1657 - val_loss: 1.0655\n",
      "Epoch 2/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7494 - val_loss: 0.9680\n",
      "Epoch 3/30\n",
      "628/628 [==============================] - 7s 10ms/step - loss: 0.7252 - val_loss: 0.9800\n",
      "Epoch 4/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7171 - val_loss: 0.9120\n",
      "Epoch 5/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7124 - val_loss: 0.9234\n",
      "Epoch 6/30\n",
      "628/628 [==============================] - 7s 10ms/step - loss: 0.7128 - val_loss: 0.9420\n",
      "Epoch 7/30\n",
      "628/628 [==============================] - 7s 12ms/step - loss: 0.7099 - val_loss: 0.9051\n",
      "Epoch 8/30\n",
      "628/628 [==============================] - 7s 10ms/step - loss: 0.7056 - val_loss: 0.9195\n",
      "Epoch 9/30\n",
      "628/628 [==============================] - 7s 12ms/step - loss: 0.7061 - val_loss: 0.9330\n",
      "Epoch 10/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7063 - val_loss: 0.9087\n",
      "Epoch 11/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7055 - val_loss: 0.8960\n",
      "Epoch 12/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7018 - val_loss: 0.9091\n",
      "Epoch 13/30\n",
      "628/628 [==============================] - 6s 9ms/step - loss: 0.6988 - val_loss: 0.9039\n",
      "Epoch 14/30\n",
      "628/628 [==============================] - 6s 9ms/step - loss: 0.7017 - val_loss: 0.9106\n",
      "Epoch 15/30\n",
      "628/628 [==============================] - 6s 10ms/step - loss: 0.6974 - val_loss: 0.9012\n",
      "Epoch 16/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.6951 - val_loss: 0.8941\n",
      "Epoch 17/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.6975 - val_loss: 0.9011\n",
      "Epoch 18/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.6966 - val_loss: 0.9032\n",
      "Epoch 19/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.6964 - val_loss: 0.9080\n",
      "Epoch 20/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.6927 - val_loss: 0.8937\n",
      "Epoch 21/30\n",
      "628/628 [==============================] - 6s 10ms/step - loss: 0.6946 - val_loss: 0.9135\n",
      "Epoch 22/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.6912 - val_loss: 0.9236\n",
      "Epoch 23/30\n",
      "628/628 [==============================] - 6s 10ms/step - loss: 0.6884 - val_loss: 0.9328\n",
      "Epoch 24/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.6875 - val_loss: 0.9167\n",
      "Epoch 25/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.6868 - val_loss: 0.9179\n",
      "73/73 [==============================] - 1s 5ms/step\n",
      "Symmetric Mean absolute percentage error: 14.29%. MASE: 1.11 MAE: 1.13\n",
      "Epoch 1/30\n",
      "628/628 [==============================] - 10s 11ms/step - loss: 1.1034 - val_loss: 1.0437\n",
      "Epoch 2/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7433 - val_loss: 0.9638\n",
      "Epoch 3/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7218 - val_loss: 0.9803\n",
      "Epoch 4/30\n",
      "628/628 [==============================] - 7s 10ms/step - loss: 0.7159 - val_loss: 0.9112\n",
      "Epoch 5/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7113 - val_loss: 0.9215\n",
      "Epoch 6/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7117 - val_loss: 0.9474\n",
      "Epoch 7/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7095 - val_loss: 0.9040\n",
      "Epoch 8/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7045 - val_loss: 0.9124\n",
      "Epoch 9/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7054 - val_loss: 0.9429\n",
      "Epoch 10/30\n",
      "628/628 [==============================] - 7s 10ms/step - loss: 0.7070 - val_loss: 0.9043\n",
      "Epoch 11/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7060 - val_loss: 0.8937\n",
      "Epoch 12/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7009 - val_loss: 0.9033\n",
      "Epoch 13/30\n",
      "628/628 [==============================] - 6s 10ms/step - loss: 0.6995 - val_loss: 0.9063\n",
      "Epoch 14/30\n",
      "628/628 [==============================] - 6s 9ms/step - loss: 0.6997 - val_loss: 0.9145\n",
      "Epoch 15/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.6981 - val_loss: 0.9012\n",
      "Epoch 16/30\n",
      "628/628 [==============================] - 7s 10ms/step - loss: 0.6941 - val_loss: 0.8906\n",
      "Epoch 17/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.6967 - val_loss: 0.8951\n",
      "Epoch 18/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.6952 - val_loss: 0.8989\n",
      "Epoch 19/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.6949 - val_loss: 0.9107\n",
      "Epoch 20/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.6932 - val_loss: 0.8943\n",
      "Epoch 21/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.6950 - val_loss: 0.9272\n",
      "73/73 [==============================] - 1s 5ms/step\n",
      "Symmetric Mean absolute percentage error: 14.61%. MASE: 1.14 MAE: 1.15\n",
      "Epoch 1/30\n",
      "628/628 [==============================] - 9s 11ms/step - loss: 1.0965 - val_loss: 1.0470\n",
      "Epoch 2/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7445 - val_loss: 0.9609\n",
      "Epoch 3/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7218 - val_loss: 0.9730\n",
      "Epoch 4/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7155 - val_loss: 0.9096\n",
      "Epoch 5/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7109 - val_loss: 0.9218\n",
      "Epoch 6/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7118 - val_loss: 0.9440\n",
      "Epoch 7/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7090 - val_loss: 0.9043\n",
      "Epoch 8/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7048 - val_loss: 0.9103\n",
      "Epoch 9/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7055 - val_loss: 0.9317\n",
      "Epoch 10/30\n",
      "628/628 [==============================] - 6s 10ms/step - loss: 0.7053 - val_loss: 0.9103\n",
      "Epoch 11/30\n",
      "628/628 [==============================] - 5s 9ms/step - loss: 0.7055 - val_loss: 0.8926\n",
      "Epoch 12/30\n",
      "628/628 [==============================] - 7s 10ms/step - loss: 0.7004 - val_loss: 0.9046\n",
      "Epoch 13/30\n",
      "628/628 [==============================] - 6s 10ms/step - loss: 0.6981 - val_loss: 0.9055\n",
      "Epoch 14/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7014 - val_loss: 0.9106\n",
      "Epoch 15/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.6986 - val_loss: 0.9045\n",
      "Epoch 16/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.6943 - val_loss: 0.8903\n",
      "Epoch 17/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.6973 - val_loss: 0.8979\n",
      "Epoch 18/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.6954 - val_loss: 0.8974\n",
      "Epoch 19/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.6950 - val_loss: 0.9173\n",
      "Epoch 20/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.6925 - val_loss: 0.8883\n",
      "Epoch 21/30\n",
      "628/628 [==============================] - 6s 10ms/step - loss: 0.6924 - val_loss: 0.9129\n",
      "Epoch 22/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.6896 - val_loss: 0.9214\n",
      "Epoch 23/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.6881 - val_loss: 0.9144\n",
      "Epoch 24/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.6875 - val_loss: 0.9105\n",
      "Epoch 25/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.6868 - val_loss: 0.9257\n",
      "73/73 [==============================] - 1s 5ms/step\n",
      "Symmetric Mean absolute percentage error: 14.39%. MASE: 1.13 MAE: 1.13\n",
      "Epoch 1/30\n",
      "628/628 [==============================] - 9s 11ms/step - loss: 1.1420 - val_loss: 1.0497\n",
      "Epoch 2/30\n",
      "628/628 [==============================] - 6s 10ms/step - loss: 0.7475 - val_loss: 0.9685\n",
      "Epoch 3/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7242 - val_loss: 0.9782\n",
      "Epoch 4/30\n",
      "628/628 [==============================] - 6s 10ms/step - loss: 0.7159 - val_loss: 0.9081\n",
      "Epoch 5/30\n",
      "628/628 [==============================] - 6s 9ms/step - loss: 0.7126 - val_loss: 0.9243\n",
      "Epoch 6/30\n",
      "628/628 [==============================] - 7s 10ms/step - loss: 0.7136 - val_loss: 0.9468\n",
      "Epoch 7/30\n",
      "628/628 [==============================] - 7s 10ms/step - loss: 0.7109 - val_loss: 0.9094\n",
      "Epoch 8/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7054 - val_loss: 0.9317\n",
      "Epoch 9/30\n",
      "628/628 [==============================] - 7s 12ms/step - loss: 0.7071 - val_loss: 0.9434\n",
      "73/73 [==============================] - 1s 7ms/step\n",
      "Symmetric Mean absolute percentage error: 15.08%. MASE: 1.19 MAE: 1.17\n",
      "Epoch 1/30\n",
      "628/628 [==============================] - 10s 11ms/step - loss: 1.1295 - val_loss: 1.0577\n",
      "Epoch 2/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7480 - val_loss: 0.9628\n",
      "Epoch 3/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7221 - val_loss: 0.9852\n",
      "Epoch 4/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7167 - val_loss: 0.9119\n",
      "Epoch 5/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7117 - val_loss: 0.9239\n",
      "Epoch 6/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7127 - val_loss: 0.9474\n",
      "Epoch 7/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7086 - val_loss: 0.9047\n",
      "Epoch 8/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7047 - val_loss: 0.9195\n",
      "Epoch 9/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7053 - val_loss: 0.9366\n",
      "Epoch 10/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7054 - val_loss: 0.9066\n",
      "Epoch 11/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7050 - val_loss: 0.8938\n",
      "Epoch 12/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.6995 - val_loss: 0.9089\n",
      "Epoch 13/30\n",
      "628/628 [==============================] - 6s 10ms/step - loss: 0.6973 - val_loss: 0.9088\n",
      "Epoch 14/30\n",
      "628/628 [==============================] - 6s 9ms/step - loss: 0.7021 - val_loss: 0.9095\n",
      "Epoch 15/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.6971 - val_loss: 0.9004\n",
      "Epoch 16/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.6932 - val_loss: 0.8927\n",
      "Epoch 17/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.6950 - val_loss: 0.8969\n",
      "Epoch 18/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.6939 - val_loss: 0.9052\n",
      "Epoch 19/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.6935 - val_loss: 0.9119\n",
      "Epoch 20/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.6906 - val_loss: 0.8946\n",
      "Epoch 21/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.6898 - val_loss: 0.9198\n",
      "73/73 [==============================] - 1s 5ms/step\n",
      "Symmetric Mean absolute percentage error: 14.57%. MASE: 1.14 MAE: 1.14\n",
      "Epoch 1/30\n",
      "628/628 [==============================] - 10s 11ms/step - loss: 1.1009 - val_loss: 1.0418\n",
      "Epoch 2/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7452 - val_loss: 0.9614\n",
      "Epoch 3/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7209 - val_loss: 0.9784\n",
      "Epoch 4/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7145 - val_loss: 0.9068\n",
      "Epoch 5/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7111 - val_loss: 0.9225\n",
      "Epoch 6/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7119 - val_loss: 0.9540\n",
      "Epoch 7/30\n",
      "628/628 [==============================] - 7s 10ms/step - loss: 0.7088 - val_loss: 0.9066\n",
      "Epoch 8/30\n",
      "628/628 [==============================] - 6s 9ms/step - loss: 0.7036 - val_loss: 0.9210\n",
      "Epoch 9/30\n",
      "628/628 [==============================] - 6s 9ms/step - loss: 0.7045 - val_loss: 0.9279\n",
      "Epoch 10/30\n",
      "628/628 [==============================] - 6s 10ms/step - loss: 0.7042 - val_loss: 0.9112\n",
      "Epoch 11/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.7034 - val_loss: 0.8929\n",
      "Epoch 12/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.6990 - val_loss: 0.9097\n",
      "Epoch 13/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.6960 - val_loss: 0.9086\n",
      "Epoch 14/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.6995 - val_loss: 0.9039\n",
      "Epoch 15/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.6955 - val_loss: 0.9023\n",
      "Epoch 16/30\n",
      "628/628 [==============================] - 7s 11ms/step - loss: 0.6927 - val_loss: 0.8949\n",
      "73/73 [==============================] - 1s 5ms/step\n",
      "Symmetric Mean absolute percentage error: 14.48%. MASE: 1.13 MAE: 1.13\n"
     ]
    }
   ],
   "source": [
    "SMAPE_values = []\n",
    "MASE_values = []\n",
    "MAE_values = []\n",
    "\n",
    "for i in range(20):\n",
    "    best_model, NN_sets = create_lstm_model(best_window, best_lstm_units, best_n_lstm_layers, best_optimizer_name, best_lr, best_dropout_value, df=df_total)\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=5)\n",
    "    history = best_model.fit(\n",
    "        NN_sets['X_train'].reshape((-1, best_window, 1)),\n",
    "        NN_sets['y_train'].reshape((-1, 1)),\n",
    "        epochs=30,\n",
    "        batch_size=16,\n",
    "        validation_data=(NN_sets['X_val'].reshape((-1, best_window, 1)), NN_sets['y_val'].reshape((-1, 1))),\n",
    "        callbacks=[early_stop]\n",
    "    )\n",
    "    y_pred = best_model.predict(NN_sets['X_test'].reshape((-1, best_window, 1)))\n",
    "    y_pred_df, SMAPE, MASE, MAE = NN_metricker(y_pred)\n",
    "    \n",
    "    # Append the metrics to the lists\n",
    "    SMAPE_values.append(SMAPE)\n",
    "    MASE_values.append(MASE)\n",
    "    MAE_values.append(MAE)\n",
    "    K.clear_session()\n",
    "    tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "\n",
    "with open('LSTM_results/LSTM_SMAPE_values.pkl', 'wb') as f:\n",
    "    pickle.dump(SMAPE_values, f)\n",
    "with open('LSTM_results/LSTM_MASE_values.pkl', 'wb') as f:\n",
    "    pickle.dump(MASE_values, f)\n",
    "with open('LSTM_results/LSTM_MAE_values.pkl', 'wb') as f:\n",
    "    pickle.dump(MAE_values, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bootstrap iterations: 100%|█████████▉| 999/1000 [00:04<00:00, 249.02it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from deepsig import aso\n",
    "\n",
    "seed = 88\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Simulate scores\n",
    "N = 88  # Number of random seeds\n",
    "\n",
    "min_eps = aso(MASE_values, MASE_values_log, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from heteroskedasticity import Heteroskedasticity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "BP = []\n",
    "GQ = []\n",
    "White = []\n",
    "het = []\n",
    "for col in df_total.columns:\n",
    "    train = df_total.iloc[:-6]\n",
    "    series = train[col]\n",
    "    test_results = Heteroskedasticity.run_all_tests(series)\n",
    "    sigl = 0.05\n",
    "    bp = test_results['Breusch-Pagan'] < sigl\n",
    "    gq = test_results['Goldfeld-Quandt'] < sigl\n",
    "    white = test_results['White'] < sigl\n",
    "    BP.append(bp)\n",
    "    GQ.append(gq)\n",
    "    White.append(white)\n",
    "    het.append(sum([bp, gq, white]) > 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21373056994818654"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(het)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_eps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for window in list(np.arange(4, 32, 4)):\n",
    "    K.clear_session()\n",
    "    tf.keras.backend.clear_session()\n",
    "    # Map the indices back to the actual values\n",
    "    best_window = window\n",
    "    best_n_lstm_layers = n_lstm_layers_values[best['n_lstm_layers']]\n",
    "    best_lstm_units = lstm_units_values[best['lstm_units']]\n",
    "    best_optimizer_name =  optimizer_values[best['optimizer']]\n",
    "    best_lr = lr_values[best['lr']]\n",
    "    best_dropout_value = dropout_values[best['dropout_value']]\n",
    "\n",
    "    best_model, NN_sets = create_lstm_model(best_window, best_lstm_units, best_n_lstm_layers, best_optimizer_name, best_lr, best_dropout_value, df=df_total)\n",
    "\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "    history = best_model.fit(\n",
    "        NN_sets['X_train'].reshape((-1, best_window, 1)),\n",
    "        NN_sets['y_train'].reshape((-1, 1)),\n",
    "        epochs=30,\n",
    "        batch_size=16,\n",
    "        validation_data=(NN_sets['X_val'].reshape((-1, best_window, 1)), NN_sets['y_val'].reshape((-1, 1))),\n",
    "        callbacks=[early_stop], verbose=0\n",
    "    )\n",
    "    y_pred = best_model.predict(NN_sets['X_test'].reshape((-1, best_window, 1)))\n",
    "    y_pred_df, SMAPE, MASE, MAE = NN_metricker(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73/73 [==============================] - 0s 1ms/step\n",
      "Symmetric Mean absolute percentage error: 14.64%. MASE: 1.15 MAE: 1.15\n",
      "73/73 [==============================] - 1s 2ms/step\n",
      "Symmetric Mean absolute percentage error: 14.40%. MASE: 1.12 MAE: 1.13\n",
      "73/73 [==============================] - 1s 3ms/step\n",
      "Symmetric Mean absolute percentage error: 14.36%. MASE: 1.12 MAE: 1.12\n",
      "73/73 [==============================] - 1s 3ms/step\n",
      "Symmetric Mean absolute percentage error: 14.31%. MASE: 1.12 MAE: 1.13\n",
      "73/73 [==============================] - 1s 4ms/step\n",
      "Symmetric Mean absolute percentage error: 14.45%. MASE: 1.13 MAE: 1.14\n",
      "73/73 [==============================] - 1s 6ms/step\n",
      "Symmetric Mean absolute percentage error: 14.44%. MASE: 1.15 MAE: 1.15\n",
      "73/73 [==============================] - 1s 6ms/step\n",
      "Symmetric Mean absolute percentage error: 15.67%. MASE: 1.24 MAE: 1.22\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize lists to store the metrics\n",
    "SMAPE_values_w = []\n",
    "MASE_values_w = []\n",
    "MAE_values_w = []\n",
    "\n",
    "# Iterate over the window sizes\n",
    "for window in list(np.arange(4, 32, 4)):\n",
    "    K.clear_session()\n",
    "    tf.keras.backend.clear_session()\n",
    "    # Map the indices back to the actual values\n",
    "    best_window = window\n",
    "    best_n_lstm_layers = n_lstm_layers_values[best['n_lstm_layers']]\n",
    "    best_lstm_units = lstm_units_values[best['lstm_units']]\n",
    "    best_optimizer_name =  optimizer_values[best['optimizer']]\n",
    "    best_lr = lr_values[best['lr']]\n",
    "    best_dropout_value = dropout_values[best['dropout_value']]\n",
    "\n",
    "    best_model, NN_sets = create_lstm_model(best_window, best_lstm_units, best_n_lstm_layers, best_optimizer_name, best_lr, best_dropout_value, df=df_total)\n",
    "\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "    history = best_model.fit(\n",
    "        NN_sets['X_train'].reshape((-1, best_window, 1)),\n",
    "        NN_sets['y_train'].reshape((-1, 1)),\n",
    "        epochs=30,\n",
    "        batch_size=16,\n",
    "        validation_data=(NN_sets['X_val'].reshape((-1, best_window, 1)), NN_sets['y_val'].reshape((-1, 1))),\n",
    "        callbacks=[early_stop], verbose=0\n",
    "    )\n",
    "    y_pred = best_model.predict(NN_sets['X_test'].reshape((-1, best_window, 1)))\n",
    "    y_pred_df, SMAPE, MASE, MAE = NN_metricker(y_pred)\n",
    "\n",
    "    # Store the metrics\n",
    "    SMAPE_values_w.append(SMAPE)\n",
    "    MASE_values_w.append(MASE)\n",
    "    MAE_values_w.append(MAE)\n",
    "\n",
    "# Plot the metrics\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(list(np.arange(4, 32, 4)), SMAPE_values_w, label='SMAPE')\n",
    "plt.plot(list(np.arange(4, 32, 4)), MASE_values_w, label='MASE')\n",
    "plt.plot(list(np.arange(4, 32, 4)), MAE_values_w, label='MAE')\n",
    "plt.xlabel('Window size')\n",
    "plt.ylabel('Metric value')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute percentage error for last values: 27.03956452%. Mean absolute scaled error for last values: 1.42855095%. SMAPE for last values: 18.97706161%. MAE for last values: 1.42654481%.\n",
      "Mean absolute percentage error for last values: 28.91958734%. Mean absolute scaled error for last values: 1.70333882%. SMAPE for last values: 22.58054107%. MAE for last values: 1.72166130%.\n",
      "Mean absolute percentage error for last values: 32.92833095%. Mean absolute scaled error for last values: 1.91044111%. SMAPE for last values: 24.41059691%. MAE for last values: 1.91454878%.\n",
      "Mean absolute percentage error for last values: 37.90876934%. Mean absolute scaled error for last values: 2.09912147%. SMAPE for last values: 26.04952890%. MAE for last values: 2.04740973%.\n",
      "Mean absolute percentage error for last values: 41.03579486%. Mean absolute scaled error for last values: 2.30599313%. SMAPE for last values: 28.27194142%. MAE for last values: 2.17955421%.\n",
      "Mean absolute percentage error for last values: 42.78287851%. Mean absolute scaled error for last values: 2.43485232%. SMAPE for last values: 29.33405775%. MAE for last values: 2.28458372%.\n",
      "Mean absolute percentage error for last values: 44.38394779%. Mean absolute scaled error for last values: 2.53008653%. SMAPE for last values: 30.55728599%. MAE for last values: 2.40333393%.\n",
      "Mean absolute percentage error for last values: 46.71421309%. Mean absolute scaled error for last values: 2.70196824%. SMAPE for last values: 32.30037555%. MAE for last values: 2.52318382%.\n",
      "Mean absolute percentage error for last values: 49.15858400%. Mean absolute scaled error for last values: 2.83612700%. SMAPE for last values: 32.44979029%. MAE for last values: 2.57900995%.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (7,) and (9,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m     smape_per_lv\u001b[38;5;241m.\u001b[39mappend(SMAPE)\n\u001b[1;32m     31\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[0;32m---> 32\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msmape_per_lv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSMAPE\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPermutation size\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     34\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMetric value\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/prophet/lib/python3.10/site-packages/matplotlib/pyplot.py:3590\u001b[0m, in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3582\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mplot)\n\u001b[1;32m   3583\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot\u001b[39m(\n\u001b[1;32m   3584\u001b[0m     \u001b[38;5;241m*\u001b[39margs: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m ArrayLike \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3588\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3589\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Line2D]:\n\u001b[0;32m-> 3590\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3591\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3592\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscalex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscalex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3593\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscaley\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaley\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3594\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3595\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3596\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/prophet/lib/python3.10/site-packages/matplotlib/axes/_axes.py:1724\u001b[0m, in \u001b[0;36mAxes.plot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1481\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1482\u001b[0m \u001b[38;5;124;03mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[1;32m   1483\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1721\u001b[0m \u001b[38;5;124;03m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[1;32m   1722\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1723\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[38;5;241m.\u001b[39mLine2D)\n\u001b[0;32m-> 1724\u001b[0m lines \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_lines(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39mdata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)]\n\u001b[1;32m   1725\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[1;32m   1726\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_line(line)\n",
      "File \u001b[0;32m~/anaconda3/envs/prophet/lib/python3.10/site-packages/matplotlib/axes/_base.py:303\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[0;34m(self, axes, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m     this \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    302\u001b[0m     args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 303\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_plot_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/prophet/lib/python3.10/site-packages/matplotlib/axes/_base.py:499\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[0;34m(self, axes, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[1;32m    496\u001b[0m     axes\u001b[38;5;241m.\u001b[39myaxis\u001b[38;5;241m.\u001b[39mupdate_units(y)\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m--> 499\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y must have same first dimension, but \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    500\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhave shapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    502\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y can be no greater than 2D, but have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    503\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (7,) and (9,)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from darts.metrics import mape, ape, mase, rmse, mse, ase, smape, mae\n",
    "def lv_model(df, train1, val1):\n",
    "    last_values = df.iloc[-4:-1, :]\n",
    "    last_values.index = pd.DatetimeIndex(['2023-01-01', '2023-04-01','2023-07-01'], dtype='datetime64[ns]', name='time', freq=None)\n",
    "    last_values_tf = TimeSeries.from_dataframe(last_values)\n",
    "    SMAPE = smape(val1, last_values_tf)\n",
    "    print(\n",
    "        \"Mean absolute percentage error for last values: {:.8f}%.\".format(\n",
    "            mape(val1, last_values_tf)),\n",
    "        \"Mean absolute scaled error for last values: {:.8f}%.\".format(\n",
    "            mase(val1, last_values_tf, train1)),\n",
    "        \"SMAPE for last values: {:.8f}%.\".format(SMAPE),\n",
    "                    \"MAE for last values: {:.8f}%.\".format(\n",
    "        mae(val_1, last_values_tf)\n",
    "    )\n",
    "        )\n",
    "    \n",
    "    return SMAPE\n",
    "smape_per_lv = []\n",
    "for x in list(np.arange(1, 20)):\n",
    "    # Create a copy of the DataFrame\n",
    "    df_train = df_total.copy()\n",
    "\n",
    "    # Permute the last 4 rows within each column\n",
    "    for col in df_train.columns:\n",
    "        df_train[col].iloc[-(3 + 4 * x):-1] = np.random.permutation(df_train[col].iloc[-(3 + 4 * x):-1])\n",
    "        \n",
    "    SMAPE = lv_model(df_train, train_1, val_1)\n",
    "    smape_per_lv.append(SMAPE)\n",
    "    \n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(list(np.arange(4, 32, 4)), smape_per_lv, label='SMAPE')\n",
    "plt.xlabel('Permutation size')\n",
    "plt.ylabel('Metric value')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prophet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
