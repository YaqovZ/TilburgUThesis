{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(54939) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "/Users/jacobzhao/anaconda3/envs/prophet/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Importing plotly failed. Interactive plots will not work.\n",
      "2024-06-15 15:58:14.778185: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import darts\n",
    "from darts import TimeSeries\n",
    "from darts.models import NaiveSeasonal, NaiveMean, NaiveDrift\n",
    "from darts.models import StatsForecastAutoARIMA, StatsForecastAutoETS, StatsForecastAutoCES, RNNModel, ExponentialSmoothing, BlockRNNModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "from darts.models import RNNModel, ExponentialSmoothing, BlockRNNModel\n",
    "from darts.metrics import mape, mase, mse, rmse, ase, ape, r2_score, smape\n",
    "from darts.utils.statistics import check_seasonality, plot_acf\n",
    "from darts.datasets import AirPassengersDataset, SunspotsDataset\n",
    "from darts.utils.timeseries_generation import datetime_attribute_timeseries\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dropout, Dense, Flatten\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import logging\n",
    "logging.disable(logging.CRITICAL)\n",
    "import random\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD, Nadam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error, mean_absolute_error\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "import os\n",
    "\n",
    "from hyperopt import base\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "os.environ['HYPEROPT_FMIN_SEED'] = \"1\"\n",
    "random.seed(88)\n",
    "np.random.seed(88)\n",
    "tf.random.set_seed(88)\n",
    "base.have_been_bugged = False  \n",
    "rstate = np.random.default_rng(88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total = pd.read_csv('time_series_thesis_question_1.csv', index_col=0)\n",
    "df_total.index = pd.to_datetime(df_total.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = TimeSeries.from_dataframe(df_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_1, val_1 = df.split_before(pd.Timestamp('20230101'))\n",
    "train_2, val_2 = df.split_before(pd.Timestamp('20230401'))\n",
    "train_3, val_3 = df.split_before(pd.Timestamp('20230701'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Xy(sliding_windows):\n",
    "    X = [[list(window[:-1]) for window in windows] for windows in sliding_windows]\n",
    "    y = [[window[-1] for window in windows] for windows in sliding_windows]\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_percentage_error, mean_absolute_error\n",
    "\n",
    "def spliter(df_total,\n",
    "            k = 4,\n",
    "            test_size = 3,\n",
    "            val_size = 3):\n",
    "    test = []\n",
    "    val = []\n",
    "    train = []\n",
    "    NN_sets = {}\n",
    "    window_size = k+1\n",
    "\n",
    "\n",
    "    for col in df_total.columns:\n",
    "        windows = [np.array(window) for window in df_total[col].rolling(window_size) if len(window) == window_size]\n",
    "        test.append(windows[-(test_size):])\n",
    "        val.append(windows[-(test_size+val_size):-(test_size)])\n",
    "        train.append(windows[:-(test_size+val_size)])\n",
    "    NN_sets['X_train'], NN_sets['y_train'] = Xy(train)\n",
    "    NN_sets['X_val'], NN_sets['y_val'] = Xy(val)\n",
    "    NN_sets['X_test'], NN_sets['y_test'] = Xy(test)\n",
    "    return NN_sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import darts\n",
    "def NN_metricker(y_pred):\n",
    "  y_pred_df = pd.DataFrame(y_pred.reshape((-1, 3)).transpose())\n",
    "  y_pred_df.columns = df_total.columns\n",
    "  y_pred_df.index = df_total.index[-3:]\n",
    "  y_pred_df.index = pd.to_datetime(y_pred_df.index)\n",
    "  y_pred_tf = TimeSeries.from_dataframe(y_pred_df)\n",
    "  SMAPE = darts.metrics.smape(val_1, y_pred_tf)\n",
    "  MASE = darts.metrics.mase(val_1, y_pred_tf, train_1)\n",
    "  MAE = darts.metrics.mae(val_1, y_pred_tf)\n",
    "  print(\n",
    "      \"Symmetric Mean absolute percentage error: {:.2f}%.\".format(\n",
    "          SMAPE),\n",
    "          \"MASE: {:.2f}\".format(MASE),\n",
    "          \"MAE: {:.2f}\".format(MAE)\n",
    "      )\n",
    "  return y_pred_df, SMAPE, MASE, MAE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, LSTM, Dense\n",
    "from tensorflow.keras.losses import Huber\n",
    "def create_cnn_lstm_model(window, lstm_layers, n_filters, optimizer_name, lr, kernel_size):\n",
    "    NN_sets = spliter(df_total, k=window) \n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    # CNN part\n",
    "    model.add(Conv1D(filters=n_filters, kernel_size=kernel_size, activation='relu'))\n",
    "    \n",
    "    for _ in range(lstm_layers):\n",
    "        model.add(LSTM(n_filters, return_sequences=True))\n",
    "    \n",
    "    # LSTM part\n",
    "    model.add(LSTM(n_filters // 2, return_sequences=False))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    optimizer_class = {'adam': Adam, 'rmsprop': RMSprop, 'sgd': SGD, 'nadam': Nadam}[optimizer_name]\n",
    "    optimizer = optimizer_class(lr)\n",
    "    model.compile(optimizer=optimizer, loss=Huber())\n",
    "    \n",
    "    return model, NN_sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-15 17:02:33.189582: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [6:31:29<00:00, 234.90s/trial, best loss: 0.9143925309181213]   \n",
      "Best hyperparameters:\n",
      "{'kernel_size': 0, 'lr': 8, 'n_filters': 1, 'n_lstm_layers': 0, 'optimizer': 2, 'window': 0}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD, Nadam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error, mean_absolute_error\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "K.clear_session()    \n",
    "# Set random seeds\n",
    "seed = 88\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# Hyperparameters to tune\n",
    "n_lstm_layers_values = [1, 2]\n",
    "n_filters_values = [8, 16, 32, 64, 128]\n",
    "optimizer_values = ['adam', 'rmsprop', 'nadam']\n",
    "window_values = [16, 20, 24, 28]\n",
    "kernel_size_values = [3, 5, 7, 9]\n",
    "lr_values = list(np.arange(1e-4, 11e-4, 1e-4))\n",
    "trials_results = []\n",
    "\n",
    "# Define the objective function for Hyperopt\n",
    "def objective(params):\n",
    "    window = int(params['window'])\n",
    "    n_lstm_layers = int(params['n_lstm_layers'])\n",
    "    n_filters = int(params['n_filters'])\n",
    "    optimizer_name = params['optimizer']\n",
    "    lr = params['lr']\n",
    "    kernel_size = int(params['kernel_size'])\n",
    "\n",
    "    # Create the CNN-LSTM model with the given hyperparameters\n",
    "    model, NN_sets = create_cnn_lstm_model(window, n_lstm_layers, n_filters, optimizer_name, lr, kernel_size)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "    X_train = NN_sets['X_train'].reshape((-1, window, 1))\n",
    "    y_train = NN_sets['y_train'].reshape((-1, 1))\n",
    "    X_val = NN_sets['X_val'].reshape((-1, window, 1))\n",
    "    y_val = NN_sets['y_val'].reshape((-1, 1))\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=30,\n",
    "        batch_size=16,\n",
    "        validation_data=(X_val, y_val),\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    val_loss = history.history['val_loss'][-1]\n",
    "    trials_results.append({'params': params, 'val_loss': val_loss})\n",
    "    return {'loss': val_loss, 'status': STATUS_OK}\n",
    "# Define the hyperparameter search space\n",
    "\n",
    "search_space = {\n",
    "    'window': hp.choice('window', window_values),\n",
    "    'n_lstm_layers': hp.choice('n_lstm_layers', n_lstm_layers_values),\n",
    "    'n_filters': hp.choice('n_filters', n_filters_values),\n",
    "    'optimizer': hp.choice('optimizer', optimizer_values),\n",
    "    'lr': hp.choice('lr', lr_values),\n",
    "    'kernel_size': hp.choice('kernel_size', kernel_size_values)\n",
    "}\n",
    "# Conduct the Bayesian optimization\n",
    "trials = Trials()\n",
    "best = fmin(objective, search_space, algo=tpe.suggest, max_evals=100, trials=trials)\n",
    "\n",
    "# Print the best hyperparameters and the corresponding validation loss\n",
    "print('Best hyperparameters:')\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming trials_results is your list of dictionaries\n",
    "df = pd.DataFrame(trials_results)\n",
    "\n",
    "# If you want to flatten the 'params' column into separate columns\n",
    "df = pd.concat([df.drop(['params'], axis=1), df['params'].apply(pd.Series)], axis=1)\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "if not os.path.exists('CNNLSTM_results'):\n",
    "    os.makedirs('CNNLSTM_results')\n",
    "\n",
    "# Write df to a csv file in the specified directory\n",
    "df.to_csv('CNNLSTM_results/CNNLSTM_tuning_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best window size: 16\n",
      "Best number of LSTM layers: 1\n",
      "Best number of filters: 16\n",
      "Best optimizer: nadam\n",
      "Best learning rate: 0.0009000000000000001\n",
      "Best kernel size: 3\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "best_window = window_values[best['window']]\n",
    "best_n_lstm_layers = n_lstm_layers_values[best['n_lstm_layers']]\n",
    "best_n_filters = n_filters_values[best['n_filters']]\n",
    "best_optimizer_name = optimizer_values[best['optimizer']]\n",
    "best_lr = lr_values[best['lr']]\n",
    "best_kernel_size = kernel_size_values[best['kernel_size']]\n",
    "\n",
    "print(f\"Best window size: {best_window}\")\n",
    "print(f\"Best number of LSTM layers: {best_n_lstm_layers}\")\n",
    "print(f\"Best number of filters: {best_n_filters}\")\n",
    "print(f\"Best optimizer: {best_optimizer_name}\")\n",
    "print(f\"Best learning rate: {best_lr}\")\n",
    "print(f\"Best kernel size: {best_kernel_size}\")\n",
    "\n",
    "best_model, NN_sets = create_cnn_lstm_model(best_window, best_n_lstm_layers, best_n_filters, best_optimizer_name, best_lr, best_kernel_size)\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "X_train = NN_sets['X_train'].reshape((-1, best_window, 1))\n",
    "y_train = NN_sets['y_train'].reshape((-1, 1))\n",
    "X_val = NN_sets['X_val'].reshape((-1, best_window, 1))\n",
    "y_val = NN_sets['y_val'].reshape((-1, 1))\n",
    "\n",
    "history = best_model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=30,\n",
    "    batch_size=16,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[early_stop],\n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73/73 [==============================] - 1s 3ms/step\n",
      "Symmetric Mean absolute percentage error: 14.65%. MASE: 1.14 MAE: 1.14\n"
     ]
    }
   ],
   "source": [
    "y_pred = best_model.predict(NN_sets['X_test'].reshape((-1, best_window, 1)))\n",
    "y_pred_df, SMAPE, MASE, MAE = NN_metricker(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73/73 [==============================] - 3s 4ms/step\n",
      "Symmetric Mean absolute percentage error: 14.79%. MASE: 1.14 MAE: 1.15\n",
      "73/73 [==============================] - 4s 4ms/step\n",
      "Symmetric Mean absolute percentage error: 14.50%. MASE: 1.12 MAE: 1.13\n",
      "73/73 [==============================] - 2s 4ms/step\n",
      "Symmetric Mean absolute percentage error: 14.61%. MASE: 1.14 MAE: 1.15\n",
      "73/73 [==============================] - 2s 3ms/step\n",
      "Symmetric Mean absolute percentage error: 14.49%. MASE: 1.13 MAE: 1.15\n",
      "73/73 [==============================] - 2s 2ms/step\n",
      "Symmetric Mean absolute percentage error: 14.53%. MASE: 1.14 MAE: 1.14\n",
      "73/73 [==============================] - 2s 4ms/step\n",
      "Symmetric Mean absolute percentage error: 14.51%. MASE: 1.12 MAE: 1.14\n",
      "73/73 [==============================] - 2s 4ms/step\n",
      "Symmetric Mean absolute percentage error: 14.49%. MASE: 1.12 MAE: 1.14\n",
      "73/73 [==============================] - 15s 3ms/step\n",
      "Symmetric Mean absolute percentage error: 14.54%. MASE: 1.12 MAE: 1.15\n",
      "73/73 [==============================] - 3s 4ms/step\n",
      "Symmetric Mean absolute percentage error: 14.45%. MASE: 1.12 MAE: 1.14\n",
      "73/73 [==============================] - 2s 2ms/step\n",
      "Symmetric Mean absolute percentage error: 14.48%. MASE: 1.13 MAE: 1.13\n",
      "73/73 [==============================] - 1s 3ms/step\n",
      "Symmetric Mean absolute percentage error: 14.63%. MASE: 1.14 MAE: 1.15\n",
      "73/73 [==============================] - 1s 3ms/step\n",
      "Symmetric Mean absolute percentage error: 14.58%. MASE: 1.14 MAE: 1.15\n",
      "73/73 [==============================] - 1s 3ms/step\n",
      "Symmetric Mean absolute percentage error: 14.38%. MASE: 1.12 MAE: 1.14\n",
      "73/73 [==============================] - 1s 2ms/step\n",
      "Symmetric Mean absolute percentage error: 14.62%. MASE: 1.14 MAE: 1.15\n",
      "73/73 [==============================] - 1s 3ms/step\n",
      "Symmetric Mean absolute percentage error: 14.81%. MASE: 1.17 MAE: 1.17\n",
      "73/73 [==============================] - 1s 3ms/step\n",
      "Symmetric Mean absolute percentage error: 14.59%. MASE: 1.13 MAE: 1.15\n",
      "73/73 [==============================] - 1s 3ms/step\n",
      "Symmetric Mean absolute percentage error: 14.64%. MASE: 1.13 MAE: 1.13\n",
      "73/73 [==============================] - 1s 2ms/step\n",
      "Symmetric Mean absolute percentage error: 14.58%. MASE: 1.14 MAE: 1.14\n",
      "73/73 [==============================] - 1s 3ms/step\n",
      "Symmetric Mean absolute percentage error: 14.61%. MASE: 1.13 MAE: 1.15\n",
      "73/73 [==============================] - 1s 3ms/step\n",
      "Symmetric Mean absolute percentage error: 14.39%. MASE: 1.12 MAE: 1.13\n"
     ]
    }
   ],
   "source": [
    "SMAPE_values = []\n",
    "MASE_values = []\n",
    "MAE_values = []\n",
    "\n",
    "for i in range(20):\n",
    "    K.clear_session()\n",
    "    tf.keras.backend.clear_session()\n",
    "    best_model, NN_sets = create_cnn_lstm_model(best_window, best_n_lstm_layers, best_n_filters, best_optimizer_name, best_lr, best_kernel_size)    \n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=5)\n",
    "    history = best_model.fit(\n",
    "        NN_sets['X_train'].reshape((-1, best_window, 1)),\n",
    "        NN_sets['y_train'].reshape((-1, 1)),\n",
    "        epochs=30,\n",
    "        batch_size=16,\n",
    "        validation_data=(NN_sets['X_val'].reshape((-1, best_window, 1)), NN_sets['y_val'].reshape((-1, 1))),\n",
    "        callbacks=[early_stop],\n",
    "        verbose=0\n",
    "    )\n",
    "    y_pred = best_model.predict(NN_sets['X_test'].reshape((-1, best_window, 1)))\n",
    "    y_pred_df, SMAPE, MASE, MAE = NN_metricker(y_pred)\n",
    "    \n",
    "    # Append the metrics to the lists\n",
    "    SMAPE_values.append(SMAPE)\n",
    "    MASE_values.append(MASE)\n",
    "    MAE_values.append(MAE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "with open('CNNLSTM_results/CNNLSTM_SMAPE_values.pkl', 'wb') as f:\n",
    "    pickle.dump(SMAPE_values, f)\n",
    "with open('CNNLSTM_results/CNNLSTM_MASE_values.pkl', 'wb') as f:\n",
    "    pickle.dump(MASE_values, f)\n",
    "with open('CNNLSTM_results/CNNLSTM_MAE_values.pkl', 'wb') as f:\n",
    "    pickle.dump(MAE_values, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prophet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
